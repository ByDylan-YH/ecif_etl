<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://192.168.1.6:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.cj.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>By9216446o6</value>
    </property>
    <property>
        <name>datanucleus.connectionPoolingType</name>
        <value>dbcp</value>
        <description>
            Expects one of [bonecp, dbcp, hikaricp, none].
            Specify connection pool library for datanucleus
        </description>
    </property>
    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>
    <property>
        <name>hive.server2.thrift.bind.host</name>
        <value>192.168.1.201</value>
    </property>
    <property>
        <name>hive.server2.enable.doAs</name>
        <value>true</value>
    </property>
    <property>
        <name>hive.server2.authentication</name>
        <value>NONE</value>
    </property>
    <!-- Hive整合 Tez -->
    <property>
        <name>hive.tez.container.size</name>
        <value>2048</value>
    </property>
    <!-- 允许 hive 递归读取子目录 -->
    <property>
        <name>mapred.input.dir.recursive</name>
        <value>true</value>
    </property>
    <!-- Hive整合 Spark -->
    <property>
        <name>spark.master</name>
        <value>spark://by202:7077</value>
    </property>
    <property>
        <name>spark.home</name>
        <value>/opt/software/spark-3.0.1-bin-hadoop3.2</value>
    </property>
    <!-- spark.yarn.jars         hdfs://192.168.1.202:9870/spark_jars/* -->
    <!-- # hdfs dfs -put /opt/software/spark-3.0.0-bin-hadoop3.2/jars/* /spark_jars/ -->
    <property>
        <name>spark.yarn.jars</name>
        <value>hdfs://192.168.1.202:9870/spark_jars/*</value>
    </property>
    <property>
        <name>hive.enable.spark.execution.engine</name>
        <value>true</value>
    </property>
    <!-- Hive整合 HBase -->
    <property>
        <name>hive.zookeeper.quorum</name>
        <value>192.168.1.201,192.168.1.202,192.168.1.203</value>
    </property>
</configuration>
